---
title: "Homework 1 - Data Science 3"
author: 'Oscar Leal - ID: 1903161'
date: "3/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = FALSE}
library(data.table)
library(magrittr)
library(caret)
library(rpart)
library(rpart.plot)
library(xgboost)
library(ranger)
library(gbm)
library(ISLR)
library(skimr)
library(ROCR)
library(tidyverse)
```

## 1. Tree ensemble models (7 points)

In this problem you are going to work with the OJ dataset from the ISLR package. This dataset records purchases of two types of orange juices and presents customer and product characteristics as features. The goal is to predict which of the juices is chosen in a given purchase situation. See ?ISLR::OJ for a description of the variables.

```{r}
data <- data.table(OJ)
skim(data)
summary(data)
```

### Create a training data of 75% and keep 25% of the data as a test set. Train a decision tree as a benchmark model. Plot the final model and interpret the result.

```{r}
training_ratio <- 0.75 
set.seed(1234)
train_indices <- createDataPartition(
  y = data[["Purchase"]],
  times = 1,
  p = training_ratio,
  list = FALSE
)

data_train <- data[train_indices, ]
data_test <- data[-train_indices, ]

set.seed(123)

train_control <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 3
)

simple_tree_model <- train(Purchase ~ .,
                      method = "rpart",
                      data = data_train,
                      tuneGrid = data.frame(cp = c(0.01, 0.02, 0.05)),
                      trControl = train_control)
simple_tree_model
```

```{r}
rpart.plot(simple_tree_model[["finalModel"]])
```

### Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation.

## Random Forest

```{r}
tune_grid <- expand.grid(
  .mtry = c(2, 3, 5, 7, 9, 12, 14),
  .splitrule = "gini",
  .min.node.size = c(5, 10)
)

# random forest
set.seed(1234)
rf_model <- train(Purchase ~ .,
                  method = "ranger",
                  data = data_train,
                  trControl = train_control,
                  tuneGrid = tune_grid,
                  importance = "impurity"
                  )
rf_model
```

## GBM

```{r}
gbm_grid <- expand.grid(n.trees = c(100, 500, 1000), 
                        interaction.depth = c(2, 3, 5), 
                        shrinkage = c(0.005, 0.01, 0.1),
                        n.minobsinnode = c(1))
set.seed(1234)
gbm_model <- train(Purchase ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model
```

# Refining GBM with values from optimal model

```{r}
# we can refine the grid around the optimum found
gbm_grid_refined <- expand.grid(
  n.trees = c(500, 1000, 1800), 
  interaction.depth = c(2, 3, 5), 
  shrinkage = c(0.0025, 0.005, 0.01, 0.02),
  n.minobsinnode = c(1)
)

set.seed(1234)
gbm_model_refined <- train(Purchase ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid_refined,
                   verbose = FALSE # gbm by default prints too much output
                   )
gbm_model_refined
```

## XGBoost

```{r}
xgb_grid <- expand.grid(nrounds = c(500, 1000),
                       max_depth = c(2, 3, 5),
                       eta = c(0.01, 0.05),
                       gamma = 0,
                       colsample_bytree = c(0.5, 0.7),
                       min_child_weight = 1, # similar to n.minobsinnode
                       subsample = c(0.5))
set.seed(1234)
xgboost_model <- train(Purchase ~ .,
                       method = "xgbTree",
                       data = data_train,
                       trControl = train_control,
                       tuneGrid = xgb_grid)
xgboost_model
```


### Compare different models with the resamples function (make sure to set the same seed before model training for all 3 models). Is any of these giving significantly different predictive power than the others?

```{r}
resamples_object <- resamples(list("rpart" = simple_tree_model,
                                   "rf" = rf_model,
                                   "gbm" = gbm_model_refined,
                                   "xgboost" = xgboost_model))
summary(resamples_object)

```

### Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC.

```{r}
library(PRROC)

data_test_prediction <- data_test %>% mutate(prediction = predict(xgboost_model, data_test))


PRROC_obj <- roc.curve(scores.class0 = data_test_prediction$prediction, weights.class0=data_test_prediction$labels,
                       curve=TRUE)
plot(PRROC_obj)

```

### Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models?

```{r}
plot(varImp(rf_model))
```

```{r}
plot(varImp(gbm_model_refined))
```

```{r}
plot(varImp(xgboost_model))
```

## 2. Variable importance profiles (6 points)

Use the Hitters dataset and predict log_salary just like we did it in class.

```{r}
data <- data.table(Hitters)
data <- data[!is.na(Salary)]
data[, log_salary := log(Salary)]
data[, Salary := NULL]

```

### Train two random forest models: one with mtry = 2 and another with mtry = 10 (use the whole dataset and donâ€™t use cross-validation). Inspect variable importance profiles. What do you see in terms of how important the first few variables are relative to each other?

### One of them is more extreme in terms of how the most important and the next ones relate to each other. Give an intuitive explanation how mtry relates to relative importance of variables in random forest models.

### In the same vein, estimate two gbm models and set bag.fraction to 0.1 first and to 0.9 in the second. The tuneGrid should consist of the same values for the two models (a dataframe with one row): n.trees = 500, interaction.depth = 5, shrinkage = 0.1, n.minobsinnode = 5. Compare variable importance plots for the two models. What is the meaning of bag.fraction? Based on this, why is one variable importance profile more extreme than the other?


